resourceMetrics:
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: default
        - key: k8s.namespace.uid
          value:
            stringValue: 14eeb889-5376-4b04-9215-a71c38912159
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The current phase of namespaces (1 for active and 0 for terminating)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.namespace.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: e2ek8scombined
        - key: k8s.namespace.uid
          value:
            stringValue: 634c223d-ab79-4ff1-aa99-6db59346ab06
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The current phase of namespaces (1 for active and 0 for terminating)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.namespace.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-node-lease
        - key: k8s.namespace.uid
          value:
            stringValue: 3935ad02-f9c7-4db7-adfc-b3a2bda32986
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The current phase of namespaces (1 for active and 0 for terminating)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.namespace.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-public
        - key: k8s.namespace.uid
          value:
            stringValue: aecd22e6-3012-4f43-a9ae-484eed6ae7a4
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The current phase of namespaces (1 for active and 0 for terminating)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.namespace.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.namespace.uid
          value:
            stringValue: 43674f97-9c10-48b9-8485-e72194c286dc
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The current phase of namespaces (1 for active and 0 for terminating)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.namespace.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: local-path-storage
        - key: k8s.namespace.uid
          value:
            stringValue: 8f7b4ec4-64e5-42f6-aeda-204ac916f774
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The current phase of namespaces (1 for active and 0 for terminating)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.namespace.phase
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.node.uid
          value:
            stringValue: 331efa97-c735-4d19-971f-f3fddcf7b65c
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The condition of a particular Node.
            gauge:
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: condition
                      value:
                        stringValue: DiskPressure
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: condition
                      value:
                        stringValue: MemoryPressure
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: condition
                      value:
                        stringValue: PIDPressure
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "1"
                  attributes:
                    - key: condition
                      value:
                        stringValue: Ready
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.node.condition
            unit: '{condition}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.node.uid
          value:
            stringValue: 331efa97-c735-4d19-971f-f3fddcf7b65c
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Ready condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "1"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_ready
          - description: MemoryPressure condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "0"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_memory_pressure
          - description: PIDPressure condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "0"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_pid_pressure
          - description: DiskPressure condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "0"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_disk_pressure
          - description: NetworkUnavailable condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "-1"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_network_unavailable
          - description: Amount of cpu allocatable on the node
            gauge:
              dataPoints:
                - asDouble: 6
                  timeUnixNano: "1000000"
            name: k8s.node.allocatable_cpu
            unit: '{cpu}'
          - description: Amount of memory allocatable on the node
            gauge:
              dataPoints:
                - asInt: "12508241920"
                  timeUnixNano: "1000000"
            name: k8s.node.allocatable_memory
            unit: By
          - description: Amount of pods allocatable on the node
            gauge:
              dataPoints:
                - asInt: "110"
                  timeUnixNano: "1000000"
            name: k8s.node.allocatable_pods
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.node.uid
          value:
            stringValue: 786ed111-025b-4c6e-8d57-a8130ed127ad
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The condition of a particular Node.
            gauge:
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: condition
                      value:
                        stringValue: DiskPressure
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: condition
                      value:
                        stringValue: MemoryPressure
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: condition
                      value:
                        stringValue: PIDPressure
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "1"
                  attributes:
                    - key: condition
                      value:
                        stringValue: Ready
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.node.condition
            unit: '{condition}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.node.uid
          value:
            stringValue: 786ed111-025b-4c6e-8d57-a8130ed127ad
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Ready condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "1"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_ready
          - description: MemoryPressure condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "0"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_memory_pressure
          - description: PIDPressure condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "0"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_pid_pressure
          - description: DiskPressure condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "0"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_disk_pressure
          - description: NetworkUnavailable condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "-1"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_network_unavailable
          - description: Amount of cpu allocatable on the node
            gauge:
              dataPoints:
                - asDouble: 6
                  timeUnixNano: "1000000"
            name: k8s.node.allocatable_cpu
            unit: '{cpu}'
          - description: Amount of memory allocatable on the node
            gauge:
              dataPoints:
                - asInt: "12508241920"
                  timeUnixNano: "1000000"
            name: k8s.node.allocatable_memory
            unit: By
          - description: Amount of pods allocatable on the node
            gauge:
              dataPoints:
                - asInt: "110"
                  timeUnixNano: "1000000"
            name: k8s.node.allocatable_pods
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.node.name
          value:
            stringValue: kind-worker2
        - key: k8s.node.uid
          value:
            stringValue: a240129d-1999-44b4-bcdc-74a3f9b34b8f
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The condition of a particular Node.
            gauge:
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: condition
                      value:
                        stringValue: DiskPressure
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: condition
                      value:
                        stringValue: MemoryPressure
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: condition
                      value:
                        stringValue: PIDPressure
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "1"
                  attributes:
                    - key: condition
                      value:
                        stringValue: Ready
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.node.condition
            unit: '{condition}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.node.name
          value:
            stringValue: kind-worker2
        - key: k8s.node.uid
          value:
            stringValue: a240129d-1999-44b4-bcdc-74a3f9b34b8f
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Ready condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "1"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_ready
          - description: MemoryPressure condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "0"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_memory_pressure
          - description: PIDPressure condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "0"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_pid_pressure
          - description: DiskPressure condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "0"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_disk_pressure
          - description: NetworkUnavailable condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "-1"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_network_unavailable
          - description: Amount of cpu allocatable on the node
            gauge:
              dataPoints:
                - asDouble: 6
                  timeUnixNano: "1000000"
            name: k8s.node.allocatable_cpu
            unit: '{cpu}'
          - description: Amount of memory allocatable on the node
            gauge:
              dataPoints:
                - asInt: "12508241920"
                  timeUnixNano: "1000000"
            name: k8s.node.allocatable_memory
            unit: By
          - description: Amount of pods allocatable on the node
            gauge:
              dataPoints:
                - asInt: "110"
                  timeUnixNano: "1000000"
            name: k8s.node.allocatable_pods
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.node.name
          value:
            stringValue: kind-worker3
        - key: k8s.node.uid
          value:
            stringValue: 37809be4-d3d9-4840-b747-cb56ca1fd4aa
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: The condition of a particular Node.
            gauge:
              dataPoints:
                - asInt: "0"
                  attributes:
                    - key: condition
                      value:
                        stringValue: DiskPressure
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: condition
                      value:
                        stringValue: MemoryPressure
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "0"
                  attributes:
                    - key: condition
                      value:
                        stringValue: PIDPressure
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
                - asInt: "1"
                  attributes:
                    - key: condition
                      value:
                        stringValue: Ready
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.node.condition
            unit: '{condition}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.node.name
          value:
            stringValue: kind-worker3
        - key: k8s.node.uid
          value:
            stringValue: 37809be4-d3d9-4840-b747-cb56ca1fd4aa
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Ready condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "1"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_ready
          - description: MemoryPressure condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "0"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_memory_pressure
          - description: PIDPressure condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "0"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_pid_pressure
          - description: DiskPressure condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "0"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_disk_pressure
          - description: NetworkUnavailable condition status of the node (true=1, false=0, unknown=-1)
            gauge:
              dataPoints:
                - asInt: "-1"
                  timeUnixNano: "1000000"
            name: k8s.node.condition_network_unavailable
          - description: Amount of cpu allocatable on the node
            gauge:
              dataPoints:
                - asDouble: 6
                  timeUnixNano: "1000000"
            name: k8s.node.allocatable_cpu
            unit: '{cpu}'
          - description: Amount of memory allocatable on the node
            gauge:
              dataPoints:
                - asInt: "12508241920"
                  timeUnixNano: "1000000"
            name: k8s.node.allocatable_memory
            unit: By
          - description: Amount of pods allocatable on the node
            gauge:
              dataPoints:
                - asInt: "110"
                  timeUnixNano: "1000000"
            name: k8s.node.allocatable_pods
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.daemonset.uid
          value:
            stringValue: 1bfa0b7c-3905-4501-a029-5c7fcb3a3957
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.workload.kind
          value:
            stringValue: daemonset
        - key: k8s.workload.name
          value:
            stringValue: kube-proxy
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Number of nodes that are running at least 1 daemon pod and are supposed to run the daemon pod
            gauge:
              dataPoints:
                - asInt: "4"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.current_scheduled_nodes
            unit: '{node}'
          - description: Number of nodes that should be running the daemon pod (including nodes currently running the daemon pod)
            gauge:
              dataPoints:
                - asInt: "4"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.desired_scheduled_nodes
            unit: '{node}'
          - description: Number of nodes that are running the daemon pod, but are not supposed to run the daemon pod
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.misscheduled_nodes
            unit: '{node}'
          - description: Number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready
            gauge:
              dataPoints:
                - asInt: "4"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.ready_nodes
            unit: '{node}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.daemonset.uid
          value:
            stringValue: 421c82b8-d95e-43bc-861f-441296c49edf
        - key: k8s.namespace.name
          value:
            stringValue: e2ek8scombined
        - key: k8s.workload.kind
          value:
            stringValue: daemonset
        - key: k8s.workload.name
          value:
            stringValue: otelcol-kosifdfp3l
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Number of nodes that are running at least 1 daemon pod and are supposed to run the daemon pod
            gauge:
              dataPoints:
                - asInt: "3"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.current_scheduled_nodes
            unit: '{node}'
          - description: Number of nodes that should be running the daemon pod (including nodes currently running the daemon pod)
            gauge:
              dataPoints:
                - asInt: "3"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.desired_scheduled_nodes
            unit: '{node}'
          - description: Number of nodes that are running the daemon pod, but are not supposed to run the daemon pod
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.misscheduled_nodes
            unit: '{node}'
          - description: Number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready
            gauge:
              dataPoints:
                - asInt: "3"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.ready_nodes
            unit: '{node}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.daemonset.uid
          value:
            stringValue: e074ac36-0d53-4592-85b5-5ed330746efe
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.workload.kind
          value:
            stringValue: daemonset
        - key: k8s.workload.name
          value:
            stringValue: kindnet
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Number of nodes that are running at least 1 daemon pod and are supposed to run the daemon pod
            gauge:
              dataPoints:
                - asInt: "4"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.current_scheduled_nodes
            unit: '{node}'
          - description: Number of nodes that should be running the daemon pod (including nodes currently running the daemon pod)
            gauge:
              dataPoints:
                - asInt: "4"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.desired_scheduled_nodes
            unit: '{node}'
          - description: Number of nodes that are running the daemon pod, but are not supposed to run the daemon pod
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.misscheduled_nodes
            unit: '{node}'
          - description: Number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready
            gauge:
              dataPoints:
                - asInt: "4"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.daemonset.ready_nodes
            unit: '{node}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.deployment.uid
          value:
            stringValue: b890fc8e-27e9-4bf8-aa4c-3572e2b41645
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.workload.kind
          value:
            stringValue: deployment
        - key: k8s.workload.name
          value:
            stringValue: coredns
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Total number of available pods (ready for at least minReadySeconds) targeted by this deployment
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.deployment.available
            unit: '{pod}'
          - description: Number of desired pods in this deployment
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.deployment.desired
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.deployment.uid
          value:
            stringValue: e7107d99-1070-40ca-8415-f3eeae279a14
        - key: k8s.namespace.name
          value:
            stringValue: local-path-storage
        - key: k8s.workload.kind
          value:
            stringValue: deployment
        - key: k8s.workload.name
          value:
            stringValue: local-path-provisioner
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Total number of available pods (ready for at least minReadySeconds) targeted by this deployment
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.deployment.available
            unit: '{pod}'
          - description: Number of desired pods in this deployment
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.deployment.desired
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.deployment.uid
          value:
            stringValue: f37a1c0d-3db3-4460-acc4-c58dd95eb4ee
        - key: k8s.namespace.name
          value:
            stringValue: e2ek8scombined
        - key: k8s.workload.kind
          value:
            stringValue: deployment
        - key: k8s.workload.name
          value:
            stringValue: otelcol-gwpqgcaiiv
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Total number of available pods (ready for at least minReadySeconds) targeted by this deployment
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.deployment.available
            unit: '{pod}'
          - description: Number of desired pods in this deployment
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.deployment.desired
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: e2ek8scombined
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: otelcol-kosifdfp3l-hsj6m
        - key: k8s.pod.uid
          value:
            stringValue: c872b3d3-2af9-4aec-bdb5-2501987019e3
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: e2ek8scombined
        - key: k8s.node.name
          value:
            stringValue: kind-worker3
        - key: k8s.pod.name
          value:
            stringValue: otelcol-kosifdfp3l-fqq6v
        - key: k8s.pod.uid
          value:
            stringValue: 0733242c-cdca-4a22-9fa7-0e9c85ed696c
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: e2ek8scombined
        - key: k8s.replicaset.uid
          value:
            stringValue: 064a40ad-354b-4bf7-90b0-70d74e81877c
        - key: k8s.workload.kind
          value:
            stringValue: replicaset
        - key: k8s.workload.name
          value:
            stringValue: otelcol-gwpqgcaiiv-6b4574d445
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Total number of available pods (ready for at least minReadySeconds) targeted by this replicaset
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.replicaset.available
            unit: '{pod}'
          - description: Number of desired pods in this replicaset
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.replicaset.desired
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: coredns-6f6b679f8f-6p6zn
        - key: k8s.pod.uid
          value:
            stringValue: 817c518d-feca-4504-a6fb-ec8b372ac7ce
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: coredns-6f6b679f8f-l8svq
        - key: k8s.pod.uid
          value:
            stringValue: adece318-f621-46c6-afb7-3f232a6e8406
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: etcd-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: 600e917f-239c-47e3-b643-357f7416dc8c
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kindnet-qnjjw
        - key: k8s.pod.uid
          value:
            stringValue: b66a4e4b-3a1a-4bdb-9d8e-df9d0cee29be
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-apiserver-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: a2dda252-ff7c-4994-ab96-2fa657fd5623
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-controller-manager-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: 2d30a349-f551-4cc0-96e7-626fad6a4eea
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-proxy-f6b8f
        - key: k8s.pod.uid
          value:
            stringValue: 46ff3678-22d5-4cdb-bd51-af9155e2684e
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-scheduler-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: e16e8923-fe5c-4904-814e-ed44edb55c45
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: kindnet-2qtgh
        - key: k8s.pod.uid
          value:
            stringValue: 3b9ad732-efab-4311-a040-c77bb00dbe7b
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: kube-proxy-4rp6b
        - key: k8s.pod.uid
          value:
            stringValue: 2be42bca-c3fa-4003-be5a-881f55e70627
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-worker3
        - key: k8s.pod.name
          value:
            stringValue: kindnet-fjzgq
        - key: k8s.pod.uid
          value:
            stringValue: dd8f2589-7b6a-4380-b1db-8aa97aebf828
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-worker3
        - key: k8s.pod.name
          value:
            stringValue: kube-proxy-ts6j5
        - key: k8s.pod.uid
          value:
            stringValue: 36234c57-1f14-4f8f-a6c7-cb3134911d64
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.replicaset.uid
          value:
            stringValue: f7b01f78-d4b7-4b3f-bb91-508f9bec161f
        - key: k8s.workload.kind
          value:
            stringValue: replicaset
        - key: k8s.workload.name
          value:
            stringValue: coredns-6f6b679f8f
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Total number of available pods (ready for at least minReadySeconds) targeted by this replicaset
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.replicaset.available
            unit: '{pod}'
          - description: Number of desired pods in this replicaset
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.replicaset.desired
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: local-path-storage
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: local-path-provisioner-57c5987fd4-rwl8k
        - key: k8s.pod.uid
          value:
            stringValue: 1d121333-4e4d-49f9-9d19-82a2d48062fa
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.namespace.name
          value:
            stringValue: local-path-storage
        - key: k8s.replicaset.uid
          value:
            stringValue: 34ce4a9a-28e5-41c5-b05f-1bae5e3a4a9c
        - key: k8s.workload.kind
          value:
            stringValue: replicaset
        - key: k8s.workload.name
          value:
            stringValue: local-path-provisioner-57c5987fd4
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Total number of available pods (ready for at least minReadySeconds) targeted by this replicaset
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.replicaset.available
            unit: '{pod}'
          - description: Number of desired pods in this replicaset
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.replicaset.desired
            unit: '{pod}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 28f574124ba1a7b051556072cd95b2f4cd288c24874a25fe210ba62e2cd7ab30
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/kube-proxy-arm64
        - key: container.image.tag
          value:
            stringValue: v1.31.0
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.container.name
          value:
            stringValue: kube-proxy
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-proxy-f6b8f
        - key: k8s.pod.uid
          value:
            stringValue: 46ff3678-22d5-4cdb-bd51-af9155e2684e
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 332a69e8ad8977fc31f856a9d5a9c3833ea607accfa1220f0b01010bb0665636
        - key: container.image.name
          value:
            stringValue: localhost/dynatrace-otel-collector
        - key: container.image.tag
          value:
            stringValue: e2e-test
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.container.name
          value:
            stringValue: opentelemetry-collector
        - key: k8s.namespace.name
          value:
            stringValue: e2ek8scombined
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: otelcol-kosifdfp3l-hsj6m
        - key: k8s.pod.uid
          value:
            stringValue: c872b3d3-2af9-4aec-bdb5-2501987019e3
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_limit
            unit: '{cpu}'
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.5
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "536870912"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_limit
            unit: By
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "536870912"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 46521a078e522577413c95ee508695ffe4475fd98dfed4bb1a0469e4e4f241b5
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/kube-apiserver-arm64
        - key: container.image.tag
          value:
            stringValue: v1.31.0
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.container.name
          value:
            stringValue: kube-apiserver
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-apiserver-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: a2dda252-ff7c-4994-ab96-2fa657fd5623
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.25
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 6545e171d44db8c9979c911dd31a4f9d2a08221420bcc3b567f6d15541efa97e
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/coredns/coredns
        - key: container.image.tag
          value:
            stringValue: v1.11.1
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.container.name
          value:
            stringValue: coredns
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: coredns-6f6b679f8f-6p6zn
        - key: k8s.pod.uid
          value:
            stringValue: 817c518d-feca-4504-a6fb-ec8b372ac7ce
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "178257920"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_limit
            unit: By
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "73400320"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 74a1360150367880a6d57599d550ec9bbea43ebde00c3b33027015d79f910f82
        - key: container.image.name
          value:
            stringValue: docker.io/kindest/kindnetd
        - key: container.image.tag
          value:
            stringValue: v20240813-c6f155d6
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.container.name
          value:
            stringValue: kindnet-cni
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: kindnet-2qtgh
        - key: k8s.pod.uid
          value:
            stringValue: 3b9ad732-efab-4311-a040-c77bb00dbe7b
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_limit
            unit: '{cpu}'
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "52428800"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_limit
            unit: By
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "52428800"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 8fd353e678cd9daacd11fb774bc1147a8266d30af4ebe7e1ad6bcf6bd97b9f79
        - key: container.image.name
          value:
            stringValue: docker.io/kindest/kindnetd
        - key: container.image.tag
          value:
            stringValue: v20240813-c6f155d6
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.container.name
          value:
            stringValue: kindnet-cni
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kindnet-qnjjw
        - key: k8s.pod.uid
          value:
            stringValue: b66a4e4b-3a1a-4bdb-9d8e-df9d0cee29be
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_limit
            unit: '{cpu}'
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "52428800"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_limit
            unit: By
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "52428800"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: a1b34a70aa186ff9ede7f5852f670316c199992b70bca5e7dea11c6a0e3d76e0
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/coredns/coredns
        - key: container.image.tag
          value:
            stringValue: v1.11.1
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.container.name
          value:
            stringValue: coredns
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: coredns-6f6b679f8f-l8svq
        - key: k8s.pod.uid
          value:
            stringValue: adece318-f621-46c6-afb7-3f232a6e8406
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "178257920"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_limit
            unit: By
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "73400320"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: a22913d75f8bd0696bd6d5284d9adc52fb6cb29f194b71cb12382e5c9eb97fae
        - key: container.image.name
          value:
            stringValue: localhost/dynatrace-otel-collector
        - key: container.image.tag
          value:
            stringValue: e2e-test
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.container.name
          value:
            stringValue: opentelemetry-collector
        - key: k8s.namespace.name
          value:
            stringValue: e2ek8scombined
        - key: k8s.node.name
          value:
            stringValue: kind-worker3
        - key: k8s.pod.name
          value:
            stringValue: otelcol-kosifdfp3l-fqq6v
        - key: k8s.pod.uid
          value:
            stringValue: 0733242c-cdca-4a22-9fa7-0e9c85ed696c
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_limit
            unit: '{cpu}'
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.5
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "536870912"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_limit
            unit: By
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "536870912"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: ac237960d05c740be09eb182c0917a29fb748e7a6dc1644c77b74e40dcf33071
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/kube-scheduler-arm64
        - key: container.image.tag
          value:
            stringValue: v1.31.0
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.container.name
          value:
            stringValue: kube-scheduler
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-scheduler-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: e16e8923-fe5c-4904-814e-ed44edb55c45
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: bc0bad09a5873d09fc7f26c31ae3f1ce57e27a7eb84cf15fc50001244398c741
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/kube-controller-manager-arm64
        - key: container.image.tag
          value:
            stringValue: v1.31.0
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.container.name
          value:
            stringValue: kube-controller-manager
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: kube-controller-manager-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: 2d30a349-f551-4cc0-96e7-626fad6a4eea
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.2
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: c4ba48251e3326a9e7f5fda425887a689964107e44f61096ff0abf8f5cd81281
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/kube-proxy-arm64
        - key: container.image.tag
          value:
            stringValue: v1.31.0
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.container.name
          value:
            stringValue: kube-proxy
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-worker3
        - key: k8s.pod.name
          value:
            stringValue: kube-proxy-ts6j5
        - key: k8s.pod.uid
          value:
            stringValue: 36234c57-1f14-4f8f-a6c7-cb3134911d64
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: cab8448b609fb999d6355603fd9f7ca58d3d40cadd7b73c3f73841022ad4fd68
        - key: container.image.name
          value:
            stringValue: docker.io/kindest/kindnetd
        - key: container.image.tag
          value:
            stringValue: v20240813-c6f155d6
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.container.name
          value:
            stringValue: kindnet-cni
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-worker3
        - key: k8s.pod.name
          value:
            stringValue: kindnet-fjzgq
        - key: k8s.pod.uid
          value:
            stringValue: dd8f2589-7b6a-4380-b1db-8aa97aebf828
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_limit
            unit: '{cpu}'
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "52428800"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_limit
            unit: By
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "52428800"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: d0e679aca4a0a16f6c88971f8f867180aa293095afedf211411c8374679a109a
        - key: container.image.name
          value:
            stringValue: docker.io/kindest/local-path-provisioner
        - key: container.image.tag
          value:
            stringValue: v20240813-c6f155d6
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.container.name
          value:
            stringValue: local-path-provisioner
        - key: k8s.namespace.name
          value:
            stringValue: local-path-storage
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: local-path-provisioner-57c5987fd4-rwl8k
        - key: k8s.pod.uid
          value:
            stringValue: 1d121333-4e4d-49f9-9d19-82a2d48062fa
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: e3e26ac3e89132ed74ad5b467f2ad80e9c14d69cc8d6c9f6be7a52f996f6072e
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/etcd
        - key: container.image.tag
          value:
            stringValue: 3.5.15-0
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.container.name
          value:
            stringValue: etcd
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-control-plane
        - key: k8s.pod.name
          value:
            stringValue: etcd-kind-control-plane
        - key: k8s.pod.uid
          value:
            stringValue: 600e917f-239c-47e3-b643-357f7416dc8c
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "104857600"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: e84cc9c8137d6680a32465452e52a76d63de0049b9f35b5ca6505df32868e74d
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/kube-proxy-arm64
        - key: container.image.tag
          value:
            stringValue: v1.31.0
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.container.name
          value:
            stringValue: kube-proxy
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-worker
        - key: k8s.pod.name
          value:
            stringValue: kube-proxy-4rp6b
        - key: k8s.pod.uid
          value:
            stringValue: 2be42bca-c3fa-4003-be5a-881f55e70627
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.cluster.uid
          value:
            stringValue: 43674f97-9c10-48b9-8485-e72194c286dc
        - key: k8s.namespace.name
          value:
            stringValue: e2ek8scombined
        - key: k8s.node.name
          value:
            stringValue: kind-worker2
        - key: k8s.pod.ip
          value:
            stringValue: 10.244.2.4
        - key: k8s.pod.name
          value:
            stringValue: otelcol-gwpqgcaiiv-6b4574d445-4zvl9
        - key: k8s.pod.uid
          value:
            stringValue: 7a544e59-b596-49d4-bda2-f78e8cdeb4df
        - key: k8s.workload.kind
          value:
            stringValue: deployment
        - key: k8s.workload.name
          value:
            stringValue: otelcol-gwpqgcaiiv
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.cluster.uid
          value:
            stringValue: 43674f97-9c10-48b9-8485-e72194c286dc
        - key: k8s.namespace.name
          value:
            stringValue: e2ek8scombined
        - key: k8s.node.name
          value:
            stringValue: kind-worker2
        - key: k8s.pod.ip
          value:
            stringValue: 10.244.2.5
        - key: k8s.pod.name
          value:
            stringValue: otelcol-kosifdfp3l-wb6f6
        - key: k8s.pod.uid
          value:
            stringValue: 95f98677-5d1d-4624-a506-9f0516f899a0
        - key: k8s.workload.kind
          value:
            stringValue: daemonset
        - key: k8s.workload.name
          value:
            stringValue: otelcol-kosifdfp3l
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.cluster.uid
          value:
            stringValue: 43674f97-9c10-48b9-8485-e72194c286dc
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-worker2
        - key: k8s.pod.ip
          value:
            stringValue: 10.89.0.21
        - key: k8s.pod.name
          value:
            stringValue: kindnet-c8wd6
        - key: k8s.pod.uid
          value:
            stringValue: 139bb070-2170-42aa-87b5-55d1b41048b2
        - key: k8s.workload.kind
          value:
            stringValue: daemonset
        - key: k8s.workload.name
          value:
            stringValue: kindnet
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.cluster.uid
          value:
            stringValue: 43674f97-9c10-48b9-8485-e72194c286dc
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-worker2
        - key: k8s.pod.ip
          value:
            stringValue: 10.89.0.21
        - key: k8s.pod.name
          value:
            stringValue: kube-proxy-mzfvx
        - key: k8s.pod.uid
          value:
            stringValue: 833c2e2d-7fce-4e1b-98b0-2a3d51ae9887
        - key: k8s.workload.kind
          value:
            stringValue: daemonset
        - key: k8s.workload.name
          value:
            stringValue: kube-proxy
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Current phase of the pod (1 - Pending, 2 - Running, 3 - Succeeded, 4 - Failed, 5 - Unknown)
            gauge:
              dataPoints:
                - asInt: "2"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.phase
          - description: Current status reason of the pod (1 - Evicted, 2 - NodeAffinity, 3 - NodeLost, 4 - Shutdown, 5 - UnexpectedAdmissionError, 6 - Unknown)
            gauge:
              dataPoints:
                - asInt: "6"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.pod.status_reason
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 5912d522a6528e19303957838942cf87c63c9e85d43ec9103ada8f2d8fc2b825
        - key: container.image.name
          value:
            stringValue: registry.k8s.io/kube-proxy-arm64
        - key: container.image.tag
          value:
            stringValue: v1.31.0
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.cluster.uid
          value:
            stringValue: 43674f97-9c10-48b9-8485-e72194c286dc
        - key: k8s.container.name
          value:
            stringValue: kube-proxy
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-worker2
        - key: k8s.pod.ip
          value:
            stringValue: 10.89.0.21
        - key: k8s.pod.name
          value:
            stringValue: kube-proxy-mzfvx
        - key: k8s.pod.uid
          value:
            stringValue: 833c2e2d-7fce-4e1b-98b0-2a3d51ae9887
        - key: k8s.workload.kind
          value:
            stringValue: daemonset
        - key: k8s.workload.name
          value:
            stringValue: kube-proxy
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: 5ce78b912370729dce2708ffd4e4373aa0e05dfab2953cc0db85199d368fbe73
        - key: container.image.name
          value:
            stringValue: localhost/dynatrace-otel-collector
        - key: container.image.tag
          value:
            stringValue: e2e-test
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.cluster.uid
          value:
            stringValue: 43674f97-9c10-48b9-8485-e72194c286dc
        - key: k8s.container.name
          value:
            stringValue: opentelemetry-collector
        - key: k8s.namespace.name
          value:
            stringValue: e2ek8scombined
        - key: k8s.node.name
          value:
            stringValue: kind-worker2
        - key: k8s.pod.ip
          value:
            stringValue: 10.244.2.5
        - key: k8s.pod.name
          value:
            stringValue: otelcol-kosifdfp3l-wb6f6
        - key: k8s.pod.uid
          value:
            stringValue: 95f98677-5d1d-4624-a506-9f0516f899a0
        - key: k8s.workload.kind
          value:
            stringValue: daemonset
        - key: k8s.workload.name
          value:
            stringValue: otelcol-kosifdfp3l
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_limit
            unit: '{cpu}'
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.5
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "536870912"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_limit
            unit: By
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "536870912"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: e417cc5cbb8e6c7a4560c9165341be4ba79b50a26f93741542d587af4cf98253
        - key: container.image.name
          value:
            stringValue: docker.io/kindest/kindnetd
        - key: container.image.tag
          value:
            stringValue: v20240813-c6f155d6
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.cluster.uid
          value:
            stringValue: 43674f97-9c10-48b9-8485-e72194c286dc
        - key: k8s.container.name
          value:
            stringValue: kindnet-cni
        - key: k8s.namespace.name
          value:
            stringValue: kube-system
        - key: k8s.node.name
          value:
            stringValue: kind-worker2
        - key: k8s.pod.ip
          value:
            stringValue: 10.89.0.21
        - key: k8s.pod.name
          value:
            stringValue: kindnet-c8wd6
        - key: k8s.pod.uid
          value:
            stringValue: 139bb070-2170-42aa-87b5-55d1b41048b2
        - key: k8s.workload.kind
          value:
            stringValue: daemonset
        - key: k8s.workload.name
          value:
            stringValue: kindnet
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_limit
            unit: '{cpu}'
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "52428800"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_limit
            unit: By
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "52428800"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
  - resource:
      attributes:
        - key: container.id
          value:
            stringValue: fdfd93ae3b0eea92421f965155c20b2e9bf0f701136e0b73cef7d9553c0bbde1
        - key: container.image.name
          value:
            stringValue: localhost/dynatrace-otel-collector
        - key: container.image.tag
          value:
            stringValue: e2e-test
        - key: k8s.cluster.name
          value:
            stringValue: k8s-testing-cluster
        - key: k8s.cluster.uid
          value:
            stringValue: 43674f97-9c10-48b9-8485-e72194c286dc
        - key: k8s.container.name
          value:
            stringValue: opentelemetry-collector
        - key: k8s.namespace.name
          value:
            stringValue: e2ek8scombined
        - key: k8s.node.name
          value:
            stringValue: kind-worker2
        - key: k8s.pod.ip
          value:
            stringValue: 10.244.2.4
        - key: k8s.pod.name
          value:
            stringValue: otelcol-gwpqgcaiiv-6b4574d445-4zvl9
        - key: k8s.pod.uid
          value:
            stringValue: 7a544e59-b596-49d4-bda2-f78e8cdeb4df
        - key: k8s.workload.kind
          value:
            stringValue: deployment
        - key: k8s.workload.name
          value:
            stringValue: otelcol-gwpqgcaiiv
    schemaUrl: https://opentelemetry.io/schemas/1.18.0
    scopeMetrics:
      - metrics:
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 1
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_limit
            unit: '{cpu}'
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asDouble: 0.5
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.cpu_request
            unit: '{cpu}'
          - description: Maximum resource limit set for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "536870912"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_limit
            unit: By
          - description: Resource requested for the container. See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#resourcerequirements-v1-core for details
            gauge:
              dataPoints:
                - asInt: "536870912"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.memory_request
            unit: By
          - description: Whether a container has passed its readiness probe (0 for no, 1 for yes)
            gauge:
              dataPoints:
                - asInt: "1"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.ready
          - description: How many times the container has restarted in the recent past. This value is pulled directly from the K8s API and the value can go indefinitely high and be reset to 0 at any time depending on how your kubelet is configured to prune dead containers. It is best to not depend too much on the exact value but rather look at it as either == 0, in which case you can conclude there were no restarts in the recent past, or > 0, in which case you can conclude there were restarts in the recent past, and not try and analyze the value beyond that.
            gauge:
              dataPoints:
                - asInt: "0"
                  startTimeUnixNano: "1000000"
                  timeUnixNano: "2000000"
            name: k8s.container.restarts
            unit: '{restart}'
        scope:
          name: github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sclusterreceiver
          version: 0.30.1
